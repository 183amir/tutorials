.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_advanced_numpy_extensions_tutorial.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_advanced_numpy_extensions_tutorial.py:


Creating Extensions Using numpy and scipy
=========================================
**Author**: `Adam Paszke <https://github.com/apaszke>`_

**Updated by**: `Adam Dziedzic <https://github.com/adam-dziedzic>`_

In this tutorial, we shall go through two tasks:

1. Create a neural network layer with no parameters.

    -  This calls into **numpy** as part of its implementation

2. Create a neural network layer that has learnable weights

    -  This calls into **SciPy** as part of its implementation


.. code-block:: default


    import torch
    from torch.autograd import Function







Parameter-less example
----------------------

This layer doesnâ€™t particularly do anything useful or mathematically
correct.

It is aptly named BadFFTFunction

**Layer Implementation**


.. code-block:: default


    from numpy.fft import rfft2, irfft2


    class BadFFTFunction(Function):
        @staticmethod
        def forward(ctx, input):
            numpy_input = input.detach().numpy()
            result = abs(rfft2(numpy_input))
            return input.new(result)

        @staticmethod
        def backward(ctx, grad_output):
            numpy_go = grad_output.numpy()
            result = irfft2(numpy_go)
            return grad_output.new(result)

    # since this layer does not have any parameters, we can
    # simply declare this as a function, rather than as an nn.Module class


    def incorrect_fft(input):
        return BadFFTFunction.apply(input)







**Example usage of the created layer:**


.. code-block:: default


    input = torch.randn(8, 8, requires_grad=True)
    result = incorrect_fft(input)
    print(result)
    result.backward(torch.randn(result.size()))
    print(input)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    tensor([[ 3.5676,  7.6277, 12.0316,  7.4520,  4.6516],
            [ 2.6259,  7.1881, 11.4271,  1.9694,  3.7176],
            [ 7.8666, 10.0252,  5.5213, 11.5493,  6.3659],
            [ 7.8461,  3.9869, 11.5400,  8.2209, 10.8777],
            [10.7354,  8.0773,  3.8458, 14.6773, 14.6100],
            [ 7.8461,  9.8101,  6.9741, 10.7464, 10.8777],
            [ 7.8666,  9.7762,  3.5603,  8.1164,  6.3659],
            [ 2.6259,  6.0260,  7.2829,  8.5443,  3.7176]],
           grad_fn=<BadFFTFunctionBackward>)
    tensor([[ 0.6084, -0.3023,  0.0898, -0.2358,  0.3460, -0.1489,  1.2369,  0.3228],
            [-0.3015, -1.7702,  1.2346,  0.5430, -0.8680,  1.9112, -0.2799, -1.0561],
            [-1.1582,  1.8501, -0.8297,  1.0523,  1.0496, -2.2164,  0.5843,  2.4357],
            [-0.6325, -0.7673,  1.0286, -0.7309, -0.0709, -0.4434,  1.1623,  0.2643],
            [-1.0950,  0.6666, -0.9408, -0.9364, -1.7739,  1.2752,  1.3204,  0.7905],
            [ 1.0473,  0.6469,  1.1859, -0.6140,  0.2747, -0.5180, -0.4766,  0.4013],
            [-0.9397,  2.2113,  1.0242, -0.4726, -0.5123,  0.9380, -0.2498,  1.1610],
            [-1.4579, -0.8077, -0.9664, -2.1408, -0.4327,  1.6076,  0.2507, -0.8074]],
           requires_grad=True)


Parametrized example
--------------------

In deep learning literature, this layer is confusingly referred
to as convolution while the actual operation is cross-correlation
(the only difference is that filter is flipped for convolution,
which is not the case for cross-correlation).

Implementation of a layer with learnable weights, where cross-correlation
has a filter (kernel) that represents weights.

The backward pass computes the gradient wrt the input and the gradient wrt the filter.


.. code-block:: default


    from numpy import flip
    import numpy as np
    from scipy.signal import convolve2d, correlate2d
    from torch.nn.modules.module import Module
    from torch.nn.parameter import Parameter


    class ScipyConv2dFunction(Function):
        @staticmethod
        def forward(ctx, input, filter, bias):
            # detach so we can cast to NumPy
            input, filter, bias = input.detach(), filter.detach(), bias.detach()
            result = correlate2d(input.numpy(), filter.numpy(), mode='valid')
            result += bias.numpy()
            ctx.save_for_backward(input, filter, bias)
            return torch.as_tensor(result, dtype=input.dtype)

        @staticmethod
        def backward(ctx, grad_output):
            grad_output = grad_output.detach()
            input, filter, bias = ctx.saved_tensors
            grad_output = grad_output.numpy()
            grad_bias = np.sum(grad_output, keepdims=True)
            grad_input = convolve2d(grad_output, filter.numpy(), mode='full')
            # the previous line can be expressed equivalently as:
            # grad_input = correlate2d(grad_output, flip(flip(filter.numpy(), axis=0), axis=1), mode='full')
            grad_filter = correlate2d(input.numpy(), grad_output, mode='valid')
            return torch.from_numpy(grad_input), torch.from_numpy(grad_filter).to(torch.float), torch.from_numpy(grad_bias).to(torch.float)


    class ScipyConv2d(Module):
        def __init__(self, filter_width, filter_height):
            super(ScipyConv2d, self).__init__()
            self.filter = Parameter(torch.randn(filter_width, filter_height))
            self.bias = Parameter(torch.randn(1, 1))

        def forward(self, input):
            return ScipyConv2dFunction.apply(input, self.filter, self.bias)








**Example usage:**


.. code-block:: default


    module = ScipyConv2d(3, 3)
    print("Filter and bias: ", list(module.parameters()))
    input = torch.randn(10, 10, requires_grad=True)
    output = module(input)
    print("Output from the convolution: ", output)
    output.backward(torch.randn(8, 8))
    print("Gradient for the input map: ", input.grad)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Filter and bias:  [Parameter containing:
    tensor([[-0.6033, -2.5517,  0.6615],
            [-0.4015,  1.1745, -1.4735],
            [ 0.9846,  0.5996, -0.8848]], requires_grad=True), Parameter containing:
    tensor([[0.2914]], requires_grad=True)]
    Output from the convolution:  tensor([[ 0.6043,  3.2527, -0.9176,  9.2007,  2.9193, -9.1039,  3.3560,  4.0675],
            [-1.7545, -2.6112, -1.9888, -2.3944,  5.8373, -2.8703,  6.0815, -0.9792],
            [ 1.8568,  0.9849, -3.7855,  2.6516, -7.5822,  1.2759, -0.9582, -2.2369],
            [-0.8632,  4.5858,  6.5325, -3.5827,  4.3068,  2.1159, -7.2695,  2.6622],
            [-2.0365, -2.2546, -0.8329,  3.0775,  5.2646,  2.1035,  3.3298,  0.4510],
            [ 6.2140,  1.3400,  0.1431,  1.6809, -2.3829,  5.5545,  0.4369,  5.1728],
            [-2.9239,  4.7323, -1.4705, -3.3467,  4.7654, -4.4163,  5.6994, -4.0618],
            [-5.5067, -1.8681,  3.2384,  5.6807, -2.0448,  2.7258, -1.7425,  7.4629]],
           grad_fn=<ScipyConv2dFunctionBackward>)
    Gradient for the input map:  tensor([[-7.8226e-01, -3.4172e+00,  6.3186e-01,  2.8466e-01, -3.2896e+00,
              2.9865e+00,  1.7380e+00,  5.1107e+00,  4.4798e-01, -4.5012e-01],
            [-1.3170e+00, -2.1972e+00, -1.2405e+00,  1.2714e+00, -1.8506e-01,
             -4.2221e+00,  1.4235e+00, -1.5397e+00,  2.7945e+00,  8.3933e-01],
            [ 2.9938e-01,  4.8622e-01, -4.7442e-01,  3.9370e+00,  1.5090e+00,
             -1.8791e+00, -1.7817e+00, -2.5698e+00,  5.0200e-02,  1.2115e+00],
            [ 1.2362e+00,  2.6675e+00, -3.9790e+00,  5.6593e-02,  1.8580e+00,
             -4.3119e-01, -4.5306e+00,  4.7176e+00, -3.8302e+00,  3.5119e-01],
            [ 1.0134e+00,  2.6440e-01,  8.6645e-02, -2.4672e+00,  4.7412e+00,
             -1.9256e+00,  3.6223e+00, -4.5224e-03,  1.3907e+00, -1.6193e+00],
            [-2.1171e-01,  4.4629e-01, -2.5806e+00, -4.5543e+00,  6.6020e+00,
              3.8461e-01, -4.5984e-01, -2.9283e+00,  4.7545e+00, -1.4404e+00],
            [-5.9877e-01, -1.3457e+00,  5.3375e+00, -9.4798e-01, -5.1292e+00,
              4.8921e+00, -4.4247e+00, -2.5115e+00, -3.1480e+00,  1.0831e+00],
            [-4.8540e-01,  1.5609e+00, -2.7854e+00,  3.8279e+00, -3.2514e+00,
             -1.6675e+00,  3.1144e+00, -2.8661e+00,  3.5903e+00, -3.2442e+00],
            [ 6.9806e-01, -1.8570e+00, -8.4111e-01,  1.1025e+00, -8.8266e-02,
             -5.5910e-01,  1.1742e+00,  2.0094e+00, -1.6131e-01, -9.5718e-01],
            [ 7.8524e-02,  7.1540e-01, -3.5772e-02, -4.0492e-01,  9.0418e-01,
             -2.8182e-01, -2.3205e-02, -2.5782e-01, -6.0938e-01,  4.8046e-01]])


**Check the gradients:**


.. code-block:: default


    from torch.autograd.gradcheck import gradcheck

    moduleConv = ScipyConv2d(3, 3)

    input = [torch.randn(20, 20, dtype=torch.double, requires_grad=True)]
    test = gradcheck(moduleConv, input, eps=1e-6, atol=1e-4)
    print("Are the gradients correct: ", test)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Are the gradients correct:  True



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.496 seconds)


.. _sphx_glr_download_advanced_numpy_extensions_tutorial.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: numpy_extensions_tutorial.py <numpy_extensions_tutorial.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: numpy_extensions_tutorial.ipynb <numpy_extensions_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
